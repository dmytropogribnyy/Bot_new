üìä P-–±–ª–æ–∫ (P0-P5) - GPT Perspectives & Audit Trail
P-–±–ª–æ–∫ - —ç—Ç–æ –Ω–∞–±–æ—Ä –º–æ–¥—É–ª–µ–π –¥–ª—è –∞—É–¥–∏—Ç–∞, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π –∏ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞. –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç "Perspectives" - —Ä–∞–∑–Ω—ã–µ "–≤–∑–≥–ª—è–¥—ã" –Ω–∞ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π.
üéØ –ó–∞—á–µ–º –Ω—É–∂–µ–Ω P-–±–ª–æ–∫:

–î–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ - –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—á–µ–º—É –±–æ—Ç –ø—Ä–∏–Ω—è–ª —Ç–æ –∏–ª–∏ –∏–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
–î–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø–æ–Ω—è—Ç—å —á—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–¥–µ–ª–∫–µ
–î–ª—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤ - –¥–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ –∑–∞—è–≤–ª–µ–Ω–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º
–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —É—Å–ø–µ—à–Ω—ã—Ö/–Ω–µ—É–¥–∞—á–Ω—ã—Ö —Å–¥–µ–ª–æ–∫

üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∞ P-–±–ª–æ–∫–∞:
pythonP0 - Audit Logger (–±–∞–∑–æ–≤—ã–π –∞—É–¥–∏—Ç)
‚îú‚îÄ‚îÄ Append-only JSONL —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ Hash-—Ü–µ–ø–æ—á–∫–∞ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îî‚îÄ‚îÄ Timestamp + —Å–æ–±—ã—Ç–∏–µ + –∫–æ–Ω—Ç–µ–∫—Å—Ç

P1 - Risk Events (—Å–æ–±—ã—Ç–∏—è —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞)
‚îú‚îÄ‚îÄ SL-streak triggered
‚îú‚îÄ‚îÄ Daily limit reached
‚îî‚îÄ‚îÄ Position size adjusted

P2 - OMS Events (—Å–æ–±—ã—Ç–∏—è –æ—Ä–¥–µ—Ä–æ–≤)
‚îú‚îÄ‚îÄ Order placed/cancelled/filled
‚îú‚îÄ‚îÄ TP/SL triggered
‚îî‚îÄ‚îÄ Slippage recorded

P3 - Decision Records (–∑–∞–ø–∏—Å—å —Ä–µ—à–µ–Ω–∏–π)
‚îú‚îÄ‚îÄ –ü–æ—á–µ–º—É –æ—Ç–∫—Ä—ã–ª–∏ –ø–æ–∑–∏—Ü–∏—é
‚îú‚îÄ‚îÄ –ö–∞–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã —Å—Ä–∞–±–æ—Ç–∞–ª–∏
‚îî‚îÄ‚îÄ –ü–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç —Ä–∞–∑–º–µ—Ä

P4 - Daily Report (–¥–Ω–µ–≤–Ω–æ–π –æ—Ç—á–µ—Ç)
‚îú‚îÄ‚îÄ –°–≤–æ–¥–∫–∞ –ø–æ —Å–¥–µ–ª–∫–∞–º
‚îú‚îÄ‚îÄ Win rate, PnL
‚îî‚îÄ‚îÄ –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏

P5 - Real-time Diagnostics (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
‚îú‚îÄ‚îÄ –ñ–∏–≤–æ–π –ø–æ—Ç–æ–∫ —Ä–µ—à–µ–Ω–∏–π
‚îú‚îÄ‚îÄ –ê–ª–µ—Ä—Ç—ã –æ–± –∞–Ω–æ–º–∞–ª–∏—è—Ö
‚îî‚îÄ‚îÄ Performance metrics
üíª –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ P3 (Decision Records):
python# core/audit_logger.py
class AuditLogger:
def record_decision(self, action: str, reasoning: dict):
"""–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º"""
record = {
"timestamp": datetime.now().isoformat(),
"action": action,
"reasoning": reasoning,
"hash": self.\_compute_hash(previous_hash)
}

        # Append-only –∑–∞–ø–∏—Å—å
        with open("data/audit.jsonl", "a") as f:
            f.write(json.dumps(record) + "\n")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±–æ—Ç–µ:

audit.record_decision(
action="OPEN_LONG",
reasoning={
"symbol": "BTC/USDC",
"signals": {
"RSI": 28,
"MACD": "bullish_cross",
"volume": "above_average"
},
"risk_check": "passed",
"position_size": 0.01,
"rationale": "Oversold conditions with bullish momentum"
}
)
üìÑ –ö–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç audit.jsonl:
json{"timestamp": "2025-08-12T10:30:45", "action": "OPEN_LONG", "symbol": "BTC/USDC", "reasoning": {"RSI": 28, "MACD": "bullish", "risk": "acceptable"}, "hash": "a4f5c2..."}
{"timestamp": "2025-08-12T10:35:12", "action": "PLACE_SL", "order_id": "123456", "price": 65000, "reasoning": {"atr_based": true, "percent": 1.5}, "hash": "b7d3e1..."}
{"timestamp": "2025-08-12T11:45:00", "action": "CLOSE_TP", "pnl": 125.50, "reasoning": {"target_reached": true, "partial": false}, "hash": "c9a2f4..."}
üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ P-–±–ª–æ–∫–∞:

–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å - –∫–∞–∂–¥–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ
–î–æ–≤–µ—Ä–∏–µ - –∏–Ω–≤–µ—Å—Ç–æ—Ä—ã –≤–∏–¥—è—Ç –ª–æ–≥–∏–∫—É —Ä–µ—à–µ–Ω–∏–π
–£–ª—É—á—à–µ–Ω–∏–µ - –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
–ó–∞—â–∏—Ç–∞ - hash-—Ü–µ–ø–æ—á–∫–∞ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ–ª–∫—É –ª–æ–≥–æ–≤
Compliance - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∞—É–¥–∏—Ç–∞

üìä –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å:
P-–±–ª–æ–∫ –ù–ï —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (—Å—Ç–∞—Ç—É—Å üî¥), –Ω–æ —ç—Ç–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—É—Å–∫ –±–æ—Ç–∞. –≠—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è:

–ü—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤
–ö–æ–ø–∏—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–ø–∏—Ç–∞–ª–æ–º

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –±–æ—Ç–∞ –±–µ–∑ P-–±–ª–æ–∫–∞, –¥–æ–±–∞–≤–∏—Ç—å –µ–≥–æ –ø–æ–∑–∂–µ –∫–æ–≥–¥–∞ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å.RetryD–¥–∞–≤–∞–π —Å–¥–µ–ª–∞–µ–ºEdit–†–∞–∑—Ä–∞–±–æ—Ç–∞–ª –ø–ª–∞–Ω –∞—É–¥–∏—Ç–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ—Ç–∞4s–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å P-–±–ª–æ–∫ (–∞—É–¥–∏—Ç –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π). –≠—Ç–æ –≤–∞–∂–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞. –î–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é P-–±–ª–æ–∫–∞ —Å P0-P5 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏.
–ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å:

core/audit_logger.py - –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –∞—É–¥–∏—Ç–∞ —Å hash-—Ü–µ–ø–æ—á–∫–æ–π
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª–∏ (order_manager, risk_guard, strategies)
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏—Ç-–ª–æ–≥–æ–≤
–¢–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã

–ù–∞—á–Ω–µ–º —Å –±–∞–∑–æ–≤–æ–≥–æ P0 –∏ P3 (—Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ), –∑–∞—Ç–µ–º –¥–æ–±–∞–≤–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.üöÄ –†–µ–∞–ª–∏–∑–∞—Ü–∏—è P-–±–ª–æ–∫–∞ (Audit & Decision Trail)
–î–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞—É–¥–∏—Ç–∞ –¥–ª—è –≤–∞—à–µ–≥–æ –±–æ—Ç–∞. –ù–∞—á–Ω–µ–º —Å –±–∞–∑–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ P0 –∏ P3, –∑–∞—Ç–µ–º –¥–æ–±–∞–≤–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ.
üìù 1. –ë–∞–∑–æ–≤—ã–π Audit Logger (P0)

```
#!/usr/bin/env python3
"""
P-Block: Audit Logger with hash-chain integrity
Provides tamper-proof logging of all trading decisions and events
"""

import hashlib
import json
import os
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Optional


class AuditLevel(Enum):
    """Audit event severity levels"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    DECISION = "DECISION"  # P3: Trading decisions
    RISK = "RISK"          # P1: Risk events
    ORDER = "ORDER"        # P2: Order events
    CRITICAL = "CRITICAL"
    ERROR = "ERROR"


class AuditLogger:
    """
    Append-only audit logger with hash-chain integrity.
    Each record contains hash of previous record, making tampering detectable.
    """

    def __init__(self, audit_dir: str = "data/audit", testnet: bool = False):
        """Initialize audit logger"""
        self.audit_dir = Path(audit_dir)
        self.audit_dir.mkdir(parents=True, exist_ok=True)

        # Separate files for testnet/prod
        env_suffix = "testnet" if testnet else "prod"
        date_str = datetime.now().strftime("%Y%m%d")

        self.audit_file = self.audit_dir / f"audit_{env_suffix}_{date_str}.jsonl"
        self.decision_file = self.audit_dir / f"decisions_{env_suffix}_{date_str}.jsonl"
        self.daily_report_file = self.audit_dir / f"daily_{env_suffix}_{date_str}.json"

        self.last_hash = self._get_last_hash()
        self.session_id = self._generate_session_id()
        self.event_counter = 0

        # Log session start
        self._write_event(
            level=AuditLevel.INFO,
            event="SESSION_START",
            data={"session_id": self.session_id, "testnet": testnet}
        )

    def _generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = datetime.now().isoformat()
        return hashlib.sha256(timestamp.encode()).hexdigest()[:12]

    def _get_last_hash(self) -> str:
        """Get hash of last record or genesis hash"""
        if not self.audit_file.exists():
            return "0" * 64  # Genesis hash

        try:
            with open(self.audit_file, 'rb') as f:
                # Seek to end and read last line
                f.seek(0, 2)  # Go to end
                file_size = f.tell()

                if file_size == 0:
                    return "0" * 64

                # Read backwards to find last complete line
                f.seek(-min(4096, file_size), 2)
                lines = f.read().decode('utf-8').strip().split('\n')

                if lines:
                    last_record = json.loads(lines[-1])
                    return last_record.get('hash', "0" * 64)
        except Exception:
            return "0" * 64

        return "0" * 64

    def _compute_hash(self, data: dict) -> str:
        """Compute SHA256 hash of record + previous hash"""
        content = json.dumps(data, sort_keys=True) + self.last_hash
        return hashlib.sha256(content.encode()).hexdigest()

    def _write_event(self, level: AuditLevel, event: str, data: Any = None) -> dict:
        """Write audit event with hash chain"""
        self.event_counter += 1

        record = {
            "timestamp": datetime.now().isoformat(),
            "session_id": self.session_id,
            "event_id": self.event_counter,
            "level": level.value,
            "event": event,
            "data": data
        }

        # Add hash chain
        record["prev_hash"] = self.last_hash
        record["hash"] = self._compute_hash(record)

        # Write to audit log
        with open(self.audit_file, 'a') as f:
            f.write(json.dumps(record) + '\n')

        # Update last hash
        self.last_hash = record["hash"]

        return record

    # P0: Basic audit events
    def log_event(self, event: str, data: Any = None, level: AuditLevel = AuditLevel.INFO):
        """Log generic audit event"""
        return self._write_event(level, event, data)

    # P1: Risk events
    def log_risk_event(self, event: str, data: dict):
        """Log risk management event"""
        return self._write_event(AuditLevel.RISK, f"RISK_{event}", data)

    def log_sl_streak(self, streak: int, symbols: list, action: str):
        """Log stop-loss streak event"""
        return self.log_risk_event("SL_STREAK", {
            "streak": streak,
            "symbols": symbols,
            "action": action,
            "timestamp": datetime.now().isoformat()
        })

    def log_daily_limit(self, loss: float, limit: float, action: str):
        """Log daily loss limit event"""
        return self.log_risk_event("DAILY_LIMIT", {
            "daily_loss": loss,
            "limit": limit,
            "action": action,
            "timestamp": datetime.now().isoformat()
        })

    def log_position_size_adjustment(self, symbol: str, original: float, adjusted: float, reason: str):
        """Log position size adjustment"""
        return self.log_risk_event("SIZE_ADJUST", {
            "symbol": symbol,
            "original_size": original,
            "adjusted_size": adjusted,
            "reason": reason
        })

    # P2: Order events
    def log_order_event(self, event: str, order: dict, metadata: dict = None):
        """Log order-related event"""
        data = {
            "order_id": order.get("id"),
            "symbol": order.get("symbol"),
            "side": order.get("side"),
            "type": order.get("type"),
            "price": order.get("price"),
            "amount": order.get("amount"),
            "status": order.get("status")
        }
        if metadata:
            data["metadata"] = metadata

        return self._write_event(AuditLevel.ORDER, f"ORDER_{event}", data)

    def log_order_placed(self, order: dict, reason: str = None):
        """Log order placement"""
        return self.log_order_event("PLACED", order, {"reason": reason})

    def log_order_filled(self, order: dict, fill_price: float = None, slippage: float = None):
        """Log order fill"""
        return self.log_order_event("FILLED", order, {
            "fill_price": fill_price,
            "slippage": slippage
        })

    def log_order_cancelled(self, order: dict, reason: str = None):
        """Log order cancellation"""
        return self.log_order_event("CANCELLED", order, {"reason": reason})

    # P3: Decision records
    def record_entry_decision(self, symbol: str, side: str, signals: dict,
                             risk_check: dict, position_size: float, rationale: str):
        """Record trading entry decision with full reasoning"""
        decision = {
            "type": "ENTRY",
            "symbol": symbol,
            "side": side,
            "signals": signals,
            "risk_check": risk_check,
            "position_size": position_size,
            "rationale": rationale,
            "timestamp": datetime.now().isoformat()
        }

        # Write to main audit
        self._write_event(AuditLevel.DECISION, "ENTRY_DECISION", decision)

        # Also write to dedicated decisions file
        with open(self.decision_file, 'a') as f:
            f.write(json.dumps(decision) + '\n')

        return decision

    def record_exit_decision(self, symbol: str, reason: str, pnl: float,
                            exit_signals: dict = None, metadata: dict = None):
        """Record exit decision"""
        decision = {
            "type": "EXIT",
            "symbol": symbol,
            "reason": reason,
            "pnl": pnl,
            "exit_signals": exit_signals,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat()
        }

        self._write_event(AuditLevel.DECISION, "EXIT_DECISION", decision)

        with open(self.decision_file, 'a') as f:
            f.write(json.dumps(decision) + '\n')

        return decision

    def record_skip_decision(self, symbol: str, reason: str, signals: dict = None):
        """Record why we skipped a potential trade"""
        decision = {
            "type": "SKIP",
            "symbol": symbol,
            "reason": reason,
            "signals": signals,
            "timestamp": datetime.now().isoformat()
        }

        self._write_event(AuditLevel.DECISION, "SKIP_DECISION", decision)
        return decision

    # P4: Daily report
    def generate_daily_report(self, stats: dict):
        """Generate daily trading report"""
        report = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "generated_at": datetime.now().isoformat(),
            "session_id": self.session_id,
            "statistics": stats,
            "events_logged": self.event_counter
        }

        # Add integrity check
        report["audit_hash"] = self.last_hash

        # Write report
        with open(self.daily_report_file, 'w') as f:
            json.dump(report, f, indent=2)

        self._write_event(AuditLevel.INFO, "DAILY_REPORT_GENERATED", {
            "report_file": str(self.daily_report_file)
        })

        return report

    # P5: Real-time diagnostics
    def log_performance_metric(self, metric: str, value: float, threshold: float = None):
        """Log performance metric for real-time monitoring"""
        data = {
            "metric": metric,
            "value": value,
            "timestamp": datetime.now().isoformat()
        }

        if threshold:
            data["threshold"] = threshold
            data["alert"] = value > threshold or value < -threshold

        return self._write_event(AuditLevel.INFO, "PERFORMANCE_METRIC", data)

    def log_anomaly(self, anomaly_type: str, description: str, severity: str = "WARNING"):
        """Log detected anomaly"""
        return self._write_event(
            AuditLevel.CRITICAL if severity == "CRITICAL" else AuditLevel.INFO,
            f"ANOMALY_{anomaly_type}",
            {"description": description, "severity": severity}
        )

    # Utility methods
    def verify_integrity(self, start_date: str = None) -> tuple[bool, list]:
        """Verify hash chain integrity of audit logs"""
        errors = []

        if not self.audit_file.exists():
            return True, []

        prev_hash = "0" * 64
        line_num = 0

        try:
            with open(self.audit_file, 'r') as f:
                for line in f:
                    line_num += 1
                    record = json.loads(line.strip())

                    # Check previous hash
                    if record.get('prev_hash') != prev_hash:
                        errors.append(f"Line {line_num}: Previous hash mismatch")

                    # Recompute hash
                    stored_hash = record['hash']
                    record_copy = record.copy()
                    del record_copy['hash']
                    del record_copy['prev_hash']

                    computed = json.dumps(record_copy, sort_keys=True) + prev_hash
                    expected_hash = hashlib.sha256(computed.encode()).hexdigest()

                    if stored_hash != expected_hash:
                        errors.append(f"Line {line_num}: Hash verification failed")

                    prev_hash = stored_hash

        except Exception as e:
            errors.append(f"Verification error: {str(e)}")

        return len(errors) == 0, errors

    def get_session_summary(self) -> dict:
        """Get summary of current session"""
        return {
            "session_id": self.session_id,
            "events_logged": self.event_counter,
            "audit_file": str(self.audit_file),
            "decision_file": str(self.decision_file),
            "last_hash": self.last_hash[:8] + "...",
            "integrity_valid": self.verify_integrity()[0]
        }


# Singleton instance
_audit_logger: Optional[AuditLogger] = None


def get_audit_logger(testnet: bool = False) -> AuditLogger:
    """Get or create audit logger singleton"""
    global _audit_logger
    if _audit_logger is None:
        _audit_logger = AuditLogger(testnet=testnet)
    return _audit_logger
```

üìä 2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ OrderManager

```
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è P-–±–ª–æ–∫–∞ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª–∏
# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã

# ============================================
# 1. core/order_manager.py - –¥–æ–±–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞:
# ============================================
from core.audit_logger import get_audit_logger

# –í –º–µ—Ç–æ–¥–µ __init__ –¥–æ–±–∞–≤–∏—Ç—å:
self.audit = get_audit_logger(testnet=self.exchange.sandbox_mode)

# –í –º–µ—Ç–æ–¥–µ open_position –¥–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è:
if order and order.get('id'):
    # Log entry decision (P3)
    self.audit.record_entry_decision(
        symbol=symbol,
        side=side,
        signals=entry_signals,  # –ü–µ—Ä–µ–¥–∞—Ç—å –∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        risk_check={
            "max_positions": len(self.positions) < self.config.max_positions,
            "risk_per_trade": position_size_usdt,
            "daily_loss": getattr(self, 'daily_loss', 0)
        },
        position_size=quantity,
        rationale=f"Signal strength sufficient, risk parameters passed"
    )

    # Log order placement (P2)
    self.audit.log_order_placed(order, reason="Entry signal triggered")

# –í –º–µ—Ç–æ–¥–µ place_tp_sl_orders –¥–æ–±–∞–≤–∏—Ç—å:
if sl_order:
    self.audit.log_order_placed(sl_order, reason="Stop loss protection")
if tp_orders:
    for tp in tp_orders:
        self.audit.log_order_placed(tp, reason="Take profit target")

# –í –º–µ—Ç–æ–¥–µ close_position –¥–æ–±–∞–≤–∏—Ç—å:
self.audit.record_exit_decision(
    symbol=position['symbol'],
    reason=reason,  # 'tp_hit', 'sl_hit', 'manual', 'signal'
    pnl=realized_pnl,
    exit_signals=exit_signals if exit_signals else None,
    metadata={
        "entry_price": position.get('entry_price'),
        "exit_price": current_price,
        "duration": position.get('duration_minutes')
    }
)

# ============================================
# 2. core/risk_guard.py - –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é:
# ============================================
from core.audit_logger import get_audit_logger

class RiskGuard:
    def __init__(self, config):
        self.audit = get_audit_logger(testnet=config.testnet)
        # existing code...

    def check_sl_streak(self, recent_trades):
        # existing check...
        if sl_streak >= self.max_sl_streak:
            self.audit.log_sl_streak(
                streak=sl_streak,
                symbols=[t['symbol'] for t in recent_sl_trades],
                action="TRADING_PAUSED"
            )
        return sl_streak < self.max_sl_streak

    def check_daily_limit(self, daily_loss):
        # existing check...
        if abs(daily_loss) >= self.daily_loss_limit:
            self.audit.log_daily_limit(
                loss=daily_loss,
                limit=self.daily_loss_limit,
                action="TRADING_STOPPED"
            )
        return abs(daily_loss) < self.daily_loss_limit

# ============================================
# 3. strategies/scalping_v1.py - –¥–æ–±–∞–≤–∏—Ç—å –≤ evaluate:
# ============================================
from core.audit_logger import get_audit_logger

def evaluate(self, market_data: dict) -> dict:
    audit = get_audit_logger()

    # existing signal calculation...

    # –ï—Å–ª–∏ —Å–∏–≥–Ω–∞–ª —Å–ª–∞–±—ã–π, –∑–∞–ø–∏—Å–∞—Ç—å –ø–æ—á–µ–º—É –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏
    if signal['action'] == 'none':
        audit.record_skip_decision(
            symbol=market_data['symbol'],
            reason="Insufficient signal strength",
            signals={
                'rsi': rsi_value,
                'macd': macd_signal,
                'volume': volume_ratio,
                'atr': atr_value
            }
        )

    return signal

# ============================================
# 4. main.py - –¥–æ–±–∞–≤–∏—Ç—å –≤ SimplifiedTradingBot:
# ============================================
from core.audit_logger import get_audit_logger

class SimplifiedTradingBot:
    def __init__(self):
        # existing init...
        self.audit = get_audit_logger(testnet=self.config.testnet)

    async def run(self):
        try:
            # existing code...

            # –í –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ —á–∞—Å–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω–∏-–æ—Ç—á–µ—Ç
            if datetime.now().minute == 0:
                await self._generate_hourly_metrics()

        except KeyboardInterrupt:
            # –ü—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            await self._generate_shutdown_report()

    async def _generate_hourly_metrics(self):
        """Log hourly performance metrics (P5)"""
        stats = await self._calculate_stats()

        self.audit.log_performance_metric(
            metric="hourly_pnl",
            value=stats.get('hourly_pnl', 0),
            threshold=100  # Alert if >$100 profit/loss per hour
        )

        self.audit.log_performance_metric(
            metric="win_rate",
            value=stats.get('win_rate', 0),
            threshold=0.3  # Alert if win rate <30%
        )

    async def _generate_shutdown_report(self):
        """Generate final report on shutdown (P4)"""
        stats = {
            "total_trades": len(self.order_manager.positions),
            "total_pnl": self.total_pnl,
            "win_rate": self.calculate_win_rate(),
            "runtime_hours": self.get_runtime_hours(),
            "positions_closed": self.positions_closed_count
        }

        self.audit.generate_daily_report(stats)
        self.logger.log(f"Audit report saved: {self.audit.get_session_summary()}")
```

3.  –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏—Ç-–ª–æ–≥–æ–≤

```
#!/usr/bin/env python3
"""
Audit Analyzer - Tool for analyzing and viewing audit logs
Provides insights into trading decisions and performance
"""

import json
import sys
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List

import pandas as pd
from tabulate import tabulate


class AuditAnalyzer:
    """Analyze audit logs and generate reports"""

    def __init__(self, audit_dir: str = "data/audit"):
        self.audit_dir = Path(audit_dir)
        self.decisions = []
        self.events = []
        self.orders = []
        self.risks = []

    def load_latest_audit(self, env: str = "prod", date: str = None):
        """Load latest audit file"""
        if date is None:
            date = datetime.now().strftime("%Y%m%d")

        audit_file = self.audit_dir / f"audit_{env}_{date}.jsonl"
        decision_file = self.audit_dir / f"decisions_{env}_{date}.jsonl"

        if not audit_file.exists():
            print(f"‚ùå Audit file not found: {audit_file}")
            return False

        # Load main audit log
        with open(audit_file, 'r') as f:
            for line in f:
                record = json.loads(line.strip())
                self.events.append(record)

                # Categorize events
                if record['level'] == 'ORDER':
                    self.orders.append(record)
                elif record['level'] == 'RISK':
                    self.risks.append(record)
                elif record['level'] == 'DECISION':
                    self.decisions.append(record)

        # Load decisions file if exists
        if decision_file.exists():
            with open(decision_file, 'r') as f:
                for line in f:
                    self.decisions.append(json.loads(line.strip()))

        print(f"‚úÖ Loaded {len(self.events)} events from {audit_file.name}")
        return True

    def verify_integrity(self) -> bool:
        """Verify hash chain integrity"""
        if not self.events:
            print("‚ùå No events loaded")
            return False

        import hashlib

        prev_hash = "0" * 64
        errors = []

        for i, record in enumerate(self.events):
            # Check previous hash
            if record.get('prev_hash') != prev_hash:
                errors.append(f"Event {i+1}: Previous hash mismatch")

            # Verify current hash
            record_copy = {k: v for k, v in record.items()
                          if k not in ['hash', 'prev_hash']}
            content = json.dumps(record_copy, sort_keys=True) + prev_hash
            expected = hashlib.sha256(content.encode()).hexdigest()

            if record.get('hash') != expected:
                errors.append(f"Event {i+1}: Hash verification failed")

            prev_hash = record.get('hash', prev_hash)

        if errors:
            print("‚ùå Integrity check failed:")
            for error in errors[:5]:  # Show first 5 errors
                print(f"  - {error}")
            return False

        print("‚úÖ Integrity check passed - audit log is tamper-free")
        return True

    def analyze_decisions(self):
        """Analyze trading decisions"""
        if not self.decisions:
            print("No decisions found")
            return

        entry_decisions = [d for d in self.decisions
                          if d.get('type') == 'ENTRY' or 'ENTRY' in d.get('event', '')]
        exit_decisions = [d for d in self.decisions
                         if d.get('type') == 'EXIT' or 'EXIT' in d.get('event', '')]
        skip_decisions = [d for d in self.decisions
                         if d.get('type') == 'SKIP' or 'SKIP' in d.get('event', '')]

        print("\nüìä DECISION ANALYSIS")
        print("=" * 60)
        print(f"Total Decisions: {len(self.decisions)}")
        print(f"‚îú‚îÄ‚îÄ Entry Decisions: {len(entry_decisions)}")
        print(f"‚îú‚îÄ‚îÄ Exit Decisions: {len(exit_decisions)}")
        print(f"‚îî‚îÄ‚îÄ Skip Decisions: {len(skip_decisions)}")

        # Analyze entry reasons
        if entry_decisions:
            print("\nüéØ Entry Decisions:")
            for decision in entry_decisions[-5:]:  # Last 5
                data = decision.get('data', decision)
                print(f"\n  Symbol: {data.get('symbol')}")
                print(f"  Side: {data.get('side')}")
                print(f"  Size: {data.get('position_size')}")
                print(f"  Rationale: {data.get('rationale', 'N/A')}")

                signals = data.get('signals', {})
                if signals:
                    print(f"  Signals:")
                    for key, value in signals.items():
                        print(f"    - {key}: {value}")

        # Analyze exit reasons
        if exit_decisions:
            print("\nüèÅ Exit Decisions:")
            exit_reasons = defaultdict(int)
            total_pnl = 0

            for decision in exit_decisions:
                data = decision.get('data', decision)
                reason = data.get('reason', 'unknown')
                exit_reasons[reason] += 1
                total_pnl += data.get('pnl', 0)

            print("\n  Exit Reasons:")
            for reason, count in exit_reasons.items():
                print(f"    - {reason}: {count}")
            print(f"\n  Total PnL from exits: ${total_pnl:.2f}")

    def analyze_risk_events(self):
        """Analyze risk management events"""
        if not self.risks:
            print("\n‚úÖ No risk events triggered")
            return

        print("\n‚ö†Ô∏è RISK EVENTS")
        print("=" * 60)

        risk_types = defaultdict(list)
        for event in self.risks:
            event_type = event['event'].replace('RISK_', '')
            risk_types[event_type].append(event)

        for risk_type, events in risk_types.items():
            print(f"\n{risk_type}: {len(events)} events")

            # Show last event details
            if events:
                last_event = events[-1]
                data = last_event.get('data', {})
                print(f"  Last occurrence: {last_event['timestamp']}")

                if risk_type == 'SL_STREAK':
                    print(f"  Streak: {data.get('streak')}")
                    print(f"  Action: {data.get('action')}")
                elif risk_type == 'DAILY_LIMIT':
                    print(f"  Loss: ${data.get('daily_loss', 0):.2f}")
                    print(f"  Limit: ${data.get('limit', 0):.2f}")
                elif risk_type == 'SIZE_ADJUST':
                    print(f"  Original: {data.get('original_size')}")
                    print(f"  Adjusted: {data.get('adjusted_size')}")
                    print(f"  Reason: {data.get('reason')}")

    def analyze_orders(self):
        """Analyze order events"""
        if not self.orders:
            print("\nüìã No orders found")
            return

        print("\nüìã ORDER ANALYSIS")
        print("=" * 60)

        order_types = defaultdict(list)
        for event in self.orders:
            order_type = event['event'].replace('ORDER_', '')
            order_types[order_type].append(event)

        # Summary
        placed = len(order_types.get('PLACED', []))
        filled = len(order_types.get('FILLED', []))
        cancelled = len(order_types.get('CANCELLED', []))

        print(f"Total Orders: {len(self.orders)}")
        print(f"‚îú‚îÄ‚îÄ Placed: {placed}")
        print(f"‚îú‚îÄ‚îÄ Filled: {filled}")
        print(f"‚îî‚îÄ‚îÄ Cancelled: {cancelled}")

        if placed > 0:
            fill_rate = (filled / placed) * 100
            print(f"\nFill Rate: {fill_rate:.1f}%")

        # Calculate slippage
        filled_orders = order_types.get('FILLED', [])
        if filled_orders:
            slippages = []
            for order in filled_orders:
                data = order.get('data', {})
                metadata = data.get('metadata', {})
                slippage = metadata.get('slippage')
                if slippage is not None:
                    slippages.append(slippage)

            if slippages:
                avg_slippage = sum(slippages) / len(slippages)
                print(f"Average Slippage: {avg_slippage:.4f}")

    def generate_timeline(self, last_hours: int = 24):
        """Generate event timeline"""
        if not self.events:
            print("No events to display")
            return

        print(f"\nüìÖ EVENT TIMELINE (Last {last_hours} hours)")
        print("=" * 60)

        cutoff = datetime.now() - timedelta(hours=last_hours)
        recent_events = []

        for event in self.events:
            timestamp_str = event.get('timestamp', '')
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str)
                    if timestamp > cutoff:
                        recent_events.append(event)
                except:
                    pass

        if not recent_events:
            print("No events in timeframe")
            return

        # Group by hour
        hourly_events = defaultdict(list)
        for event in recent_events:
            timestamp = datetime.fromisoformat(event['timestamp'])
            hour_key = timestamp.strftime("%Y-%m-%d %H:00")
            hourly_events[hour_key].append(event)

        for hour, events in sorted(hourly_events.items()):
            print(f"\n{hour} ({len(events)} events)")

            # Count event types
            event_counts = defaultdict(int)
            for event in events:
                event_type = event['event'].split('_')[0]
                event_counts[event_type] += 1

            for event_type, count in event_counts.items():
                print(f"  {event_type}: {count}")

    def export_to_csv(self, output_dir: str = "data/audit/reports"):
        """Export audit data to CSV for further analysis"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Export decisions
        if self.decisions:
            df = pd.DataFrame(self.decisions)
            csv_file = output_path / f"decisions_{datetime.now().strftime('%Y%m%d')}.csv"
            df.to_csv(csv_file, index=False)
            print(f"‚úÖ Exported decisions to {csv_file}")

        # Export orders
        if self.orders:
            order_data = []
            for order in self.orders:
                data = order.get('data', {})
                order_data.append({
                    'timestamp': order.get('timestamp'),
                    'event': order.get('event'),
                    'symbol': data.get('symbol'),
                    'side': data.get('side'),
                    'type': data.get('type'),
                    'price': data.get('price'),
                    'amount': data.get('amount'),
                    'status': data.get('status')
                })

            df = pd.DataFrame(order_data)
            csv_file = output_path / f"orders_{datetime.now().strftime('%Y%m%d')}.csv"
            df.to_csv(csv_file, index=False)
            print(f"‚úÖ Exported orders to {csv_file}")


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description='Analyze audit logs')
    parser.add_argument('--env', choices=['prod', 'testnet'],
                       default='prod', help='Environment')
    parser.add_argument('--date', help='Date in YYYYMMDD format')
    parser.add_argument('--verify', action='store_true',
                       help='Verify integrity')
    parser.add_argument('--export', action='store_true',
                       help='Export to CSV')
    parser.add_argument('--timeline', type=int, default=24,
                       help='Timeline hours to show')

    args = parser.parse_args()

    analyzer = AuditAnalyzer()

    if not analyzer.load_latest_audit(env=args.env, date=args.date):
        return 1

    if args.verify:
        analyzer.verify_integrity()

    # Run all analyses
    analyzer.analyze_decisions()
    analyzer.analyze_risk_events()
    analyzer.analyze_orders()
    analyzer.generate_timeline(last_hours=args.timeline)

    if args.export:
        analyzer.export_to_csv()

    # Summary
    print("\n" + "=" * 60)
    print("üìä AUDIT SUMMARY")
    print("=" * 60)
    print(f"Total Events: {len(analyzer.events)}")
    print(f"Decisions Made: {len(analyzer.decisions)}")
    print(f"Orders Processed: {len(analyzer.orders)}")
    print(f"Risk Events: {len(analyzer.risks)}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
```

üß™ 4. –¢–µ—Å—Ç—ã –¥–ª—è P-–±–ª–æ–∫–∞

```
#!/usr/bin/env python3
"""
Unit tests for Audit Logger (P-block)
"""

import json
import tempfile
from pathlib import Path

import pytest

from core.audit_logger import AuditLogger, AuditLevel


class TestAuditLogger:
    """Test audit logger functionality"""

    @pytest.fixture
    def temp_audit_dir(self):
        """Create temporary directory for audit logs"""
        with tempfile.TemporaryDirectory() as tmpdir:
            yield tmpdir

    @pytest.fixture
    def audit_logger(self, temp_audit_dir):
        """Create audit logger instance"""
        return AuditLogger(audit_dir=temp_audit_dir, testnet=True)

    def test_initialization(self, audit_logger, temp_audit_dir):
        """Test logger initialization"""
        assert audit_logger.session_id
        assert audit_logger.event_counter > 0  # Should have logged SESSION_START

        # Check files created
        audit_dir = Path(temp_audit_dir)
        assert any(audit_dir.glob("audit_testnet_*.jsonl"))
        assert any(audit_dir.glob("decisions_testnet_*.jsonl"))

    def test_hash_chain(self, audit_logger):
        """Test hash chain integrity"""
        # Log some events
        audit_logger.log_event("TEST_EVENT_1", {"data": "test1"})
        audit_logger.log_event("TEST_EVENT_2", {"data": "test2"})
        audit_logger.log_event("TEST_EVENT_3", {"data": "test3"})

        # Verify integrity
        is_valid, errors = audit_logger.verify_integrity()
        assert is_valid
        assert len(errors) == 0

    def test_hash_chain_tampering_detection(self, audit_logger, temp_audit_dir):
        """Test that tampering is detected"""
        # Log events
        audit_logger.log_event("TEST_EVENT", {"data": "original"})

        # Tamper with the file
        audit_file = audit_logger.audit_file

        # Read all lines
        with open(audit_file, 'r') as f:
            lines = f.readlines()

        # Modify data in last line
        last_record = json.loads(lines[-1])
        last_record['data']['data'] = "tampered"
        lines[-1] = json.dumps(last_record) + '\n'

        # Write back
        with open(audit_file, 'w') as f:
            f.writelines(lines)

        # Verify should fail
        is_valid, errors = audit_logger.verify_integrity()
        assert not is_valid
        assert len(errors) > 0

    def test_decision_recording(self, audit_logger):
        """Test decision recording"""
        # Record entry decision
        decision = audit_logger.record_entry_decision(
            symbol="BTC/USDT",
            side="long",
            signals={"rsi": 30, "macd": "bullish"},
            risk_check={"max_positions": True, "daily_loss": False},
            position_size=0.01,
            rationale="Oversold with bullish divergence"
        )

        assert decision['type'] == 'ENTRY'
        assert decision['symbol'] == 'BTC/USDT'
        assert decision['signals']['rsi'] == 30

        # Check decision file
        assert audit_logger.decision_file.exists()

        with open(audit_logger.decision_file, 'r') as f:
            content = f.read()
            assert 'BTC/USDT' in content
            assert 'Oversold with bullish divergence' in content

    def test_risk_events(self, audit_logger):
        """Test risk event logging"""
        # Log SL streak
        audit_logger.log_sl_streak(
            streak=3,
            symbols=["BTC/USDT", "ETH/USDT", "XRP/USDT"],
            action="PAUSE_TRADING"
        )

        # Log daily limit
        audit_logger.log_daily_limit(
            loss=500.0,
            limit=400.0,
            action="STOP_TRADING"
        )

        # Check events were logged
        assert audit_logger.event_counter >= 3  # SESSION_START + 2 risk events

    def test_order_events(self, audit_logger):
        """Test order event logging"""
        order = {
            "id": "12345",
            "symbol": "BTC/USDT",
            "side": "buy",
            "type": "limit",
            "price": 50000,
            "amount": 0.01,
            "status": "open"
        }

        # Log order placed
        audit_logger.log_order_placed(order, reason="Entry signal")

        # Log order filled
        audit_logger.log_order_filled(order, fill_price=49995, slippage=5)

        # Verify events
        with open(audit_logger.audit_file, 'r') as f:
            content = f.read()
            assert 'ORDER_PLACED' in content
            assert 'ORDER_FILLED' in content
            assert '12345' in content

    def test_daily_report(self, audit_logger, temp_audit_dir):
        """Test daily report generation"""
        stats = {
            "total_trades": 10,
            "winning_trades": 6,
            "total_pnl": 250.50,
            "win_rate": 0.60,
            "max_drawdown": -150.0
        }

        report = audit_logger.generate_daily_report(stats)

        assert report['statistics'] == stats
        assert report['session_id'] == audit_logger.session_id
        assert audit_logger.daily_report_file.exists()

        # Load and verify report
        with open(audit_logger.daily_report_file, 'r') as f:
            saved_report = json.load(f)
            assert saved_report['statistics']['total_trades'] == 10
            assert saved_report['statistics']['win_rate'] == 0.60

    def test_performance_metrics(self, audit_logger):
        """Test performance metric logging"""
        # Log metric without threshold
        audit_logger.log_performance_metric("latency_ms", 125.5)

        # Log metric with threshold (should trigger alert)
        audit_logger.log_performance_metric("slippage", 15.0, threshold=10.0)

        # Check logs
        with open(audit_logger.audit_file, 'r') as f:
            content = f.read()
            assert 'PERFORMANCE_METRIC' in content
            assert 'latency_ms' in content
            assert '"alert": true' in content  # Should have alert for slippage

    def test_anomaly_detection(self, audit_logger):
        """Test anomaly logging"""
        audit_logger.log_anomaly(
            anomaly_type="PRICE_SPIKE",
            description="BTC price jumped 10% in 1 minute",
            severity="WARNING"
        )

        audit_logger.log_anomaly(
            anomaly_type="CONNECTION_LOSS",
            description="Lost connection to exchange for 30 seconds",
            severity="CRITICAL"
        )

        # Verify
        with open(audit_logger.audit_file, 'r') as f:
            content = f.read()
            assert 'ANOMALY_PRICE_SPIKE' in content
            assert 'ANOMALY_CONNECTION_LOSS' in content

    def test_session_summary(self, audit_logger):
        """Test session summary generation"""
        # Log some events
        audit_logger.log_event("TEST1", {"data": 1})
        audit_logger.log_event("TEST2", {"data": 2})

        summary = audit_logger.get_session_summary()

        assert summary['session_id'] == audit_logger.session_id
        assert summary['events_logged'] >= 3  # SESSION_START + 2 test events
        assert summary['integrity_valid'] == True
        assert 'audit_file' in summary
        assert 'decision_file' in summary


def test_singleton_pattern():
    """Test that get_audit_logger returns singleton"""
    from core.audit_logger import get_audit_logger, _audit_logger

    # Get first instance
    logger1 = get_audit_logger(testnet=True)
    session1 = logger1.session_id

    # Get second instance - should be same
    logger2 = get_audit_logger(testnet=True)
    session2 = logger2.session_id

    assert logger1 is logger2
    assert session1 == session2
```

üìö 5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

# üìä P-Block (Audit & Decision Trail) - –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

## üéØ –ß—Ç–æ —Ç–∞–∫–æ–µ P-–±–ª–æ–∫?

P-–±–ª–æ–∫ - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∞—É–¥–∏—Ç–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è:

-   **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å** - –∫–∞–∂–¥–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∞–Ω–æ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–æ
-   **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å** - hash-—Ü–µ–ø–æ—á–∫–∞ –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –ø–æ–¥–¥–µ–ª–∫–∏ –ª–æ–≥–æ–≤
-   **–ê–Ω–∞–ª–∏–∑** - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
-   **Compliance** - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∞—É–¥–∏—Ç–∞

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
P0 - –ë–∞–∑–æ–≤—ã–π –∞—É–¥–∏—Ç (hash-chain, —Å–æ–±—ã—Ç–∏—è)
‚îú‚îÄ‚îÄ P1 - Risk —Å–æ–±—ã—Ç–∏—è (SL-streak, –ª–∏–º–∏—Ç—ã)
‚îú‚îÄ‚îÄ P2 - Order —Å–æ–±—ã—Ç–∏—è (placed, filled, cancelled)
‚îú‚îÄ‚îÄ P3 - Decision –∑–∞–ø–∏—Å–∏ (entry/exit –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è)
‚îú‚îÄ‚îÄ P4 - Daily –æ—Ç—á–µ—Ç—ã (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –º–µ—Ç—Ä–∏–∫–∏)
‚îî‚îÄ‚îÄ P5 - Real-time –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–∞–Ω–æ–º–∞–ª–∏–∏, –∞–ª–µ—Ä—Ç—ã)
```

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
data/audit/
‚îú‚îÄ‚îÄ audit_prod_20250812.jsonl       # –û—Å–Ω–æ–≤–Ω–æ–π –∞—É–¥–∏—Ç-–ª–æ–≥
‚îú‚îÄ‚îÄ decisions_prod_20250812.jsonl   # –†–µ—à–µ–Ω–∏—è –æ —Å–¥–µ–ª–∫–∞—Ö
‚îú‚îÄ‚îÄ daily_prod_20250812.json        # –î–Ω–µ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
‚îî‚îÄ‚îÄ reports/                         # CSV —ç–∫—Å–ø–æ—Ä—Ç—ã
    ‚îú‚îÄ‚îÄ decisions_20250812.csv
    ‚îî‚îÄ‚îÄ orders_20250812.csv
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã P-–±–ª–æ–∫–∞
python -m pytest tests/test_audit_logger.py -v

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∞—É–¥–∏—Ç –≤–∫–ª—é—á–µ–Ω
python -c "from core.audit_logger import get_audit_logger; print(get_audit_logger().get_session_summary())"
```

### 2. –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å –∞—É–¥–∏—Ç–æ–º

–ê—É–¥–∏—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞:

```bash
# Testnet
BINANCE_TESTNET=true python main.py

# Production
BINANCE_TESTNET=false python main.py
```

### 3. –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤

```bash
# –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
python tools/audit_analyzer.py

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
python tools/audit_analyzer.py --verify

# –ê–Ω–∞–ª–∏–∑ testnet –ª–æ–≥–æ–≤
python tools/audit_analyzer.py --env testnet

# –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
python tools/audit_analyzer.py --export

# Timeline –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 48 —á–∞—Å–æ–≤
python tools/audit_analyzer.py --timeline 48
```

## üìä –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø–∏—Å–µ–π

### Entry Decision (P3)

```json
{
    "timestamp": "2025-08-12T10:30:45",
    "type": "ENTRY",
    "symbol": "BTC/USDC",
    "side": "long",
    "signals": {
        "rsi": 28,
        "macd": "bullish_cross",
        "volume": 1.5,
        "atr": 0.02
    },
    "risk_check": {
        "max_positions": true,
        "daily_loss": false,
        "risk_per_trade": 50.0
    },
    "position_size": 0.01,
    "rationale": "Oversold conditions with bullish momentum"
}
```

### Risk Event (P1)

```json
{
    "event": "RISK_SL_STREAK",
    "data": {
        "streak": 3,
        "symbols": ["BTC/USDC", "ETH/USDC", "SOL/USDC"],
        "action": "TRADING_PAUSED",
        "timestamp": "2025-08-12T11:45:00"
    }
}
```

### Order Event (P2)

```json
{
    "event": "ORDER_FILLED",
    "data": {
        "order_id": "123456",
        "symbol": "BTC/USDC",
        "side": "buy",
        "price": 65000,
        "amount": 0.01,
        "metadata": {
            "fill_price": 65005,
            "slippage": 5
        }
    }
}
```

## üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ—à–µ–Ω–∏–π

```python
# –í Python –∫–æ–Ω—Å–æ–ª–∏
from pathlib import Path
import json

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è
decisions_file = Path("data/audit/decisions_prod_20250812.jsonl")
decisions = []

with open(decisions_file) as f:
    for line in f:
        decisions.append(json.loads(line))

# –ê–Ω–∞–ª–∏–∑
entries = [d for d in decisions if d['type'] == 'ENTRY']
exits = [d for d in decisions if d['type'] == 'EXIT']

print(f"Entries: {len(entries)}")
print(f"Exits: {len(exits)}")

# Win rate
wins = [e for e in exits if e.get('pnl', 0) > 0]
print(f"Win Rate: {len(wins)/len(exits)*100:.1f}%")
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏

```python
from core.audit_logger import AuditLogger

logger = AuditLogger()
is_valid, errors = logger.verify_integrity()

if is_valid:
    print("‚úÖ Audit log integrity verified")
else:
    print(f"‚ùå Integrity errors: {errors}")
```

## üõ°Ô∏è –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### Hash-—Ü–µ–ø–æ—á–∫–∞

–ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å —Å–æ–¥–µ—Ä–∂–∏—Ç:

-   `prev_hash` - —Ö–µ—à –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–ø–∏—Å–∏
-   `hash` - SHA256 —Ö–µ—à —Ç–µ–∫—É—â–µ–π –∑–∞–ø–∏—Å–∏ + prev_hash

–≠—Ç–æ –¥–µ–ª–∞–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º:

-   –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π
-   –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π
-   –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –∑–∞–ø–∏—Å–µ–π

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏

```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
python tools/audit_analyzer.py --verify

# –†—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
python -c "
from core.audit_logger import AuditLogger
logger = AuditLogger()
valid, errors = logger.verify_integrity()
print('Valid' if valid else f'Errors: {errors}')
"
```

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∏ KPI

P-–±–ª–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç:

-   **Trading Metrics**

    -   Win rate
    -   Average PnL per trade
    -   Risk/reward ratio
    -   Slippage statistics

-   **Risk Metrics**

    -   SL streak count
    -   Daily drawdown
    -   Position size adjustments
    -   Risk violations

-   **System Metrics**
    -   Order fill rate
    -   API latency
    -   Connection stability
    -   Anomaly detection

## üö® –ê–ª–µ—Ä—Ç—ã –∏ –∞–Ω–æ–º–∞–ª–∏–∏

P-–±–ª–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç:

```python
# –ü—Ä–∏–º–µ—Ä –∏–∑ –∫–æ–¥–∞
audit.log_anomaly(
    anomaly_type="PRICE_SPIKE",
    description="BTC jumped 10% in 1 minute",
    severity="WARNING"
)
```

–¢–∏–ø—ã –∞–Ω–æ–º–∞–ª–∏–π:

-   `PRICE_SPIKE` - —Ä–µ–∑–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
-   `CONNECTION_LOSS` - –ø–æ—Ç–µ—Ä—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
-   `RATE_LIMIT` - –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤ API
-   `BALANCE_MISMATCH` - –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±–∞–ª–∞–Ω—Å–∞
-   `ORDER_REJECTION` - –º–∞—Å—Å–æ–≤—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ä–¥–µ—Ä–æ–≤

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ª–æ–≥–æ–≤

```python
# –í main.py –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥–µ
from core.audit_logger import AuditLogger

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
audit = AuditLogger(
    audit_dir="/path/to/audit/logs",
    testnet=False
)
```

### –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞—É–¥–∏—Ç–∞ (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```python
# –°–æ–∑–¥–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É
class DummyAuditLogger:
    def __getattr__(self, name):
        return lambda *args, **kwargs: None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É
audit = DummyAuditLogger()
```

## üìä –î–Ω–µ–≤–Ω—ã–µ –æ—Ç—á–µ—Ç—ã

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –±–æ—Ç–∞:

```json
{
    "date": "2025-08-12",
    "session_id": "a4f5c2d8b1e3",
    "statistics": {
        "total_trades": 15,
        "winning_trades": 9,
        "total_pnl": 450.75,
        "win_rate": 0.6,
        "max_drawdown": -120.5,
        "sharpe_ratio": 1.85
    },
    "audit_hash": "b7d3e1f4a2c6..."
}
```

## üîÑ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö

### –í CSV –¥–ª—è Excel

```bash
python tools/audit_analyzer.py --export
```

### –í JSON –¥–ª—è API

```python
import json
from pathlib import Path

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–Ω–µ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
report_file = Path("data/audit/daily_prod_20250812.json")
with open(report_file) as f:
    report = json.load(f)

# –û—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ –≤–Ω–µ—à–Ω–∏–π API
import requests
requests.post("https://your-api.com/reports", json=report)
```

## ‚ùì FAQ

**Q: –°–∫–æ–ª—å–∫–æ –º–µ—Å—Ç–∞ –∑–∞–Ω–∏–º–∞—é—Ç –ª–æ–≥–∏?**
A: –ü—Ä–∏–º–µ—Ä–Ω–æ 10-50 MB –≤ –¥–µ–Ω—å –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–µ.

**Q: –ú–æ–∂–Ω–æ –ª–∏ —É–¥–∞–ª—è—Ç—å —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏?**
A: –î–∞, –ø–æ—Å–ª–µ —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ö—Ä–∞–Ω–∏—Ç—å –º–∏–Ω–∏–º—É–º 30 –¥–Ω–µ–π.

**Q: –ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ integrity check failed?**
A: –ù–µ –ø–∞–Ω–∏–∫–æ–≤–∞—Ç—å. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –±—ã–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏. Hash-—Ü–µ–ø–æ—á–∫–∞ –æ—á–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞.

**Q: –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –Ω–∞–ª–æ–≥–æ–≤?**
A: –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ CSV –∏ –ø–µ—Ä–µ–¥–∞—Ç—å –±—É—Ö–≥–∞–ª—Ç–µ—Ä—É. P-–±–ª–æ–∫ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ —Å–¥–µ–ª–∫–∏ —Å timestamp –∏ PnL.

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å P-–±–ª–æ–∫–æ–º:

1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ—Å—Ç—ã: `pytest tests/test_audit_logger.py`
2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏: `tail -f data/audit/audit_*.jsonl`
3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: `python tools/audit_analyzer.py --verify`

---

**P-–±–ª–æ–∫ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!** üöÄ

‚ö° 6. –°–∫—Ä–∏–ø—Ç –±—ã—Å—Ç—Ä–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```
#!/usr/bin/env python3
"""
Setup and verify P-Block installation
Run this after adding audit_logger.py to your project
"""

import sys
from pathlib import Path


def check_installation():
    """Check if P-block is properly installed"""
    print("üîç Checking P-Block installation...")

    issues = []

    # Check if audit_logger.py exists
    audit_logger_path = Path("core/audit_logger.py")
    if not audit_logger_path.exists():
        issues.append("‚ùå core/audit_logger.py not found")
    else:
        print("‚úÖ core/audit_logger.py found")

    # Check if analyzer exists
    analyzer_path = Path("tools/audit_analyzer.py")
    if not analyzer_path.exists():
        issues.append("‚ùå tools/audit_analyzer.py not found")
    else:
        print("‚úÖ tools/audit_analyzer.py found")

    # Check if test exists
    test_path = Path("tests/test_audit_logger.py")
    if not test_path.exists():
        issues.append("‚ö†Ô∏è tests/test_audit_logger.py not found (optional)")
    else:
        print("‚úÖ tests/test_audit_logger.py found")

    # Try to import
    try:
        from core.audit_logger import get_audit_logger
        print("‚úÖ Successfully imported audit_logger")
    except ImportError as e:
        issues.append(f"‚ùå Cannot import audit_logger: {e}")

    # Check directories
    audit_dir = Path("data/audit")
    if not audit_dir.exists():
        audit_dir.mkdir(parents=True, exist_ok=True)
        print("üìÅ Created data/audit directory")
    else:
        print("‚úÖ data/audit directory exists")

    return issues


def test_basic_functionality():
    """Test basic P-block functionality"""
    print("\nüß™ Testing basic functionality...")

    try:
        from core.audit_logger import get_audit_logger

        # Get logger instance
        logger = get_audit_logger(testnet=True)
        print(f"‚úÖ Logger initialized with session: {logger.session_id[:8]}...")

        # Test logging
        logger.log_event("TEST_SETUP", {"message": "P-block setup test"})
        print("‚úÖ Successfully logged test event")

        # Test decision recording
        decision = logger.record_entry_decision(
            symbol="TEST/USDT",
            side="long",
            signals={"test": True},
            risk_check={"passed": True},
            position_size=0.01,
            rationale="Setup test"
        )
        print("‚úÖ Successfully recorded test decision")

        # Test integrity
        is_valid, errors = logger.verify_integrity()
        if is_valid:
            print("‚úÖ Integrity check passed")
        else:
            print(f"‚ùå Integrity check failed: {errors}")
            return False

        # Get summary
        summary = logger.get_session_summary()
        print(f"‚úÖ Session summary: {summary['events_logged']} events logged")

        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False


def print_integration_instructions():
    """Print instructions for integration"""
    print("\nüìù Integration Instructions:")
    print("=" * 60)
    print("""
1. Add to core/order_manager.py:

   from core.audit_logger import get_audit_logger

   # In __init__:
   self.audit = get_audit_logger(testnet=self.exchange.sandbox_mode)

   # After successful order:
   self.audit.log_order_placed(order, reason="Entry signal")

2. Add to strategies/scalping_v1.py:

   from core.audit_logger import get_audit_logger

   # In evaluate method when skipping:
   audit = get_audit_logger()
   audit.record_skip_decision(symbol, reason, signals)

3. Add to main.py:

   from core.audit_logger import get_audit_logger

   # In __init__:
   self.audit = get_audit_logger(testnet=self.config.testnet)

   # On shutdown:
   self.audit.generate_daily_report(stats)

4. Run tests:
   python -m pytest tests/test_audit_logger.py -v

5. Analyze logs:
   python tools/audit_analyzer.py --verify
""")


def main():
    """Main setup function"""
    print("üöÄ P-Block Setup and Verification")
    print("=" * 60)

    # Check installation
    issues = check_installation()

    if issues:
        print("\n‚ö†Ô∏è Installation Issues Found:")
        for issue in issues:
            print(f"  {issue}")

        if any("not found" in issue and "core/audit_logger.py" in issue for issue in issues):
            print("\n‚ùå Critical files missing. Please ensure all P-block files are added.")
            return 1

    # Test functionality
    if not test_basic_functionality():
        print("\n‚ùå Functionality tests failed")
        return 1

    # Print integration instructions
    print_integration_instructions()

    print("\n" + "=" * 60)
    print("‚úÖ P-Block is ready to use!")
    print("\nNext steps:")
    print("1. Integrate with your existing modules (see instructions above)")
    print("2. Run the bot: python main.py")
    print("3. Analyze logs: python tools/audit_analyzer.py")

    return 0


if __name__ == "__main__":
    sys.exit(main())
```

‚úÖ P-–±–ª–æ–∫ –≥–æ—Ç–æ–≤ –∫ —É—Å—Ç–∞–Ω–æ–≤–∫–µ!
–Ø —Å–æ–∑–¥–∞–ª –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞—É–¥–∏—Ç–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π (P-–±–ª–æ–∫) –¥–ª—è –≤–∞—à–µ–≥–æ –±–æ—Ç–∞. –í–æ—Ç —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–æ:
üì¶ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:

core/audit_logger.py - –û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å —Å hash-—Ü–µ–ø–æ—á–∫–æ–π
tools/audit_analyzer.py - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤
tests/test_audit_logger.py - –¢–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
docs/P_BLOCK_USAGE.md - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
setup_p_block.py - –°–∫—Ä–∏–ø—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏

üöÄ –ë—ã—Å—Ç—Ä–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞:

```
# 1. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤—ã—à–µ –≤ –ø—Ä–æ–µ–∫—Ç

# 2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
pip install pandas tabulate

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É —É—Å—Ç–∞–Ω–æ–≤–∫–∏
python setup_p_block.py

# 4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç—ã
python -m pytest tests/test_audit_logger.py -v
```

üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è):
–î–æ–±–∞–≤—å—Ç–µ –≤ main.py:

```
from core.audit_logger import get_audit_logger

# –í __init__:
self.audit = get_audit_logger(testnet=self.config.testnet)

# –ü—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ –≤ order_manager:
self.audit.record_exit_decision(
    symbol=symbol,
    reason="tp_hit",  # –∏–ª–∏ "sl_hit", "manual"
    pnl=realized_pnl
)
```

üìä –ß—Ç–æ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ:

–ö–∞–∂–¥–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∞–Ω–æ - –ø–æ—á–µ–º—É –æ—Ç–∫—Ä—ã–ª–∏/–∑–∞–∫—Ä—ã–ª–∏ –ø–æ–∑–∏—Ü–∏—é
–ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–¥–¥–µ–ª–∫–∏ - hash-—Ü–µ–ø–æ—á–∫–∞ –∫–∞–∫ –≤ –±–ª–æ–∫—á–µ–π–Ω–µ
–ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏ - win rate, PnL, –ø–∞—Ç—Ç–µ—Ä–Ω—ã
–î–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ - –ø—Ä–æ–∑—Ä–∞—á–Ω–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å
–û—Ç–ª–∞–¥–∫–∞ - –ø–æ–Ω–∏–º–∞–Ω–∏–µ —á—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫

üéØ –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞:
üìä DECISION ANALYSIS
==================================================
Total Decisions: 47
‚îú‚îÄ‚îÄ Entry Decisions: 15
‚îú‚îÄ‚îÄ Exit Decisions: 12
‚îî‚îÄ‚îÄ Skip Decisions: 20

üéØ Entry Decisions:
Symbol: BTC/USDC
Side: long
Size: 0.01
Rationale: Oversold with bullish divergence
Signals: - RSI: 28 - MACD: bullish_cross

‚úÖ Integrity check passed - audit log is tamper-free
‚ö° –ö–æ–º–∞–Ω–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã:
bash# –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–æ–≤
python tools/audit_analyzer.py

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏

python tools/audit_analyzer.py --verify

# –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV –¥–ª—è Excel

python tools/audit_analyzer.py --export

# Timeline –∑–∞ 48 —á–∞—Å–æ–≤

python tools/audit_analyzer.py --timeline 48
P-–±–ª–æ–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤! –¢–µ–ø–µ—Ä—å –≤–∞—à –±–æ—Ç –±—É–¥–µ—Ç –≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã–π –∞—É–¥–∏—Ç –≤—Å–µ—Ö —Ä–µ—à–µ–Ω–∏–π —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
